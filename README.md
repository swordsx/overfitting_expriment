# overfitting_expriment
一个关于线性回归过拟合的实验

关于过拟合的说法一般都是，特征太多了，模型太复杂了，需要正则化，或者删特征。

本实验好奇的地方在于：

加特征行吗？

思路：造几个样本，y = 2 * x1 + 3 * x2 + 高斯噪声

然后造一堆没用的特征去拟合，但去掉真正有用的x2，显然会过拟合，测试样本上面loss爆炸。

然后在不删没用特征的前提下，加上x2，观察效果。


下面是一些图：

## A、只用x1，显然欠拟合。

（x1权重1.5225）

epoch = 500, loss = 7.829764, loss_test = 11.826150, weights = tensor([[1.5225]])

![image](https://raw.githubusercontent.com/swordsx/overfitting_expriment/master/photo_2020-07-24_01-54-03.jpg)

## B、用x1、x2，不用无关特征，最正常的结果。

（x1权重1.9274，x2权重3.0155）

epoch = 500, loss = 0.001693, loss_test = 0.005986, weights = tensor([[1.9274, 3.0155]])

![image](https://raw.githubusercontent.com/swordsx/overfitting_expriment/master/photo_2020-07-24_01-52-07.jpg)

## C、不用x2，用x1、x1的一堆高次项（2次-9次）、若干无用随机特征

训练样本loss还算比较低，测试样本持续爆炸，x1高次项权重绝对值基本>100，x1本身权重-23.9504，啥也没学到。

epoch = 1000000, loss = 0.244478, loss_test = 518096.000000, weights = tensor([[ -23.9504,   29.2102,    6.8150,    1.1432,  -17.9261,    2.4492,
         -293.9969,  214.6166,  198.5907,  158.7796,   60.6364,  -38.1811,
         -132.0322, -215.8176]])
         
![image](https://raw.githubusercontent.com/swordsx/overfitting_expriment/master/photo_2020-07-24_01-39-26.jpg)

## D、在C的基础上把x2加回来。

训练样本直接到0，当然还是过拟合了，但测试样本loss不高，0.876791，x1权重1.6849，x2权重3.0236，貌似还是学到了些东西的，至少x2权重比较正常，x1如果没有受到高次项影响可能会更靠近2。

而且无关特征的权重也自动就变小了。

epoch = 100000, loss = 0.000000, loss_test = 0.876791, weights = tensor([[ 1.6849e+00,  3.0236e+00,  3.3014e-02, -2.9698e-02, -5.1292e-04,
         -2.5274e-02, -2.0225e-02,  2.8892e-01,  1.3914e-01,  1.5759e-01,
         -5.2790e-03,  1.0181e-03, -2.0989e-01, -1.4688e-01, -9.3029e-02]])

![image](https://raw.githubusercontent.com/swordsx/overfitting_expriment/master/photo_2020-07-24_01-48-12.jpg)

## E、最后试了一下在C的基础加weight_decay的正则化方法

不管weight_decay设为多少，训练样本loss也并降不下来，虽然测试样本没有爆炸得那么夸张了。

epoch = 10000, loss = 3.626639, loss_test = 313.126862, weights = tensor([[ 0.2978,  2.4629,  1.2599,  1.4709, -0.1479, -0.1019,  2.1484,  1.2876,
          0.4143, -0.3008, -0.9111, -1.4277, -1.8782, -2.2773]])
          
![image](https://raw.githubusercontent.com/swordsx/overfitting_expriment/master/photo_2020-07-24_01-59-52.jpg)

## F、去掉C里的x1高次项

测试样本loss 160左右收敛，训练loss 0，过拟合

## G、F的基础上把x2加回来

测试样本loss 95左右收敛，训练loss 0，过拟合，比D烂



实验之前的思考：

所谓过拟合，是模型把噪声当规律学到了。但我们以为的噪声，未必就是真的噪声，可能是由于未观测到的特征所影响的。

根据中心极限定理（？我也不知道我“根据”得对不对），大量独立随机变量的作用叠加在一起，服从正态分布。

那么，如果能发现更多的好特征，可能所谓的噪声，也就不再那么噪了？

如果真正有用的强特征还没有被发现，那么删特征、正则化等等方法，不见得就有帮助。

而离经叛道地“加特征”了之后，即使不正则，其他的权重是不是也会自动变小呢？

通过以上实验，不负责任地得出一个结论：好像真的可以诶。。。
